{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration in the US\n",
    "\n",
    "## Udacity Data Engineering Capstone Project\n",
    "\n",
    "### Project Summary\n",
    "The US National Tourism and Trade Office provides a database of reports on the Visitor Arrivals Program (I-94 Record). Each report contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries). Data sources include: Overseas DHS/CBP I-94 Program data; Canadian visitation data (Stats Canada) and Mexican visitation data (Banco de Mexico) (source: https://travel.trade.gov/research/reports/i94/historical/2016.html).\n",
    "\n",
    "The goal of this project is to build an ETL pipeline using the above plus supplementary sources (temperature, city demographics, airport codes) in order to build a database that can be used to derive insights on immigration patterns to the US. These insights include, but are not limited to:\n",
    "\n",
    "* location where most visitors are coming from\n",
    "* location where most visitors are headed to\n",
    "* if temperature, city demographics, etc. are a factor in destination\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, split, expr\n",
    "from pyspark.sql.functions import year, month, dayofmonth, dayofweek \n",
    "from pyspark.sql.types import StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project, we will create an analytics database by loading in each of the data sets above, cleaning and exploring the data, creating our fact and dimension tables, and running queries to discover insights as discussed above. This project uses Apache Spark to load the data. We will also be running some data quality checks to ensure our pipeline runs as expected.\n",
    "\n",
    "#### Describe and Gather Data\n",
    "\n",
    "Below is a list of the data sets we are using:\n",
    "\n",
    "1. I94 Immigration Data: This data comes from the US National Tourism and Trade Office and can be found [here](https://travel.trade.gov/research/reports/i94/historical/2016.html).\n",
    "\n",
    "2. World Temperature Data: This dataset comes from Kaggle and can be found [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "\n",
    "3. US City Demographic Data: This dataset comes from OpenSoft and can be found [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "\n",
    "4. Airport Code Table: This data comes from DataHub and can be found [here](https://datahub.io/core/airport-codes#data).\n",
    "\n",
    "We will now load in each data set in order to take an initial look. We will also provide a data dictionary (column name + description) of each data set to get a better understanding of each data set in its raw form before we begin and cleaning or transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Immigration\n",
    "\n",
    "This is a massive data set. We will load in the sample CSV file to get a feel for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|4084316.0|2016.0|   4.0| 209.0| 209.0|    HHW|20566.0|    1.0|     HI|20573.0|  61.0|    2.0|  1.0|20160422|    null| null|      G|      O|   null|      M| 1955.0|07202016|     F|  null|     JL|5.6582674633E10|00782|      WT|\n",
      "|4422636.0|2016.0|   4.0| 582.0| 582.0|    MCA|20567.0|    1.0|     TX|20568.0|  26.0|    2.0|  1.0|20160423|     MTR| null|      G|      R|   null|      M| 1990.0|10222016|     M|  null|    *GA| 9.436199593E10|XBLNG|      B2|\n",
      "|1195600.0|2016.0|   4.0| 148.0| 112.0|    OGG|20551.0|    1.0|     FL|20571.0|  76.0|    2.0|  1.0|20160407|    null| null|      G|      O|   null|      M| 1940.0|07052016|     M|  null|     LH|5.5780468433E10|00464|      WT|\n",
      "|5291768.0|2016.0|   4.0| 297.0| 297.0|    LOS|20572.0|    1.0|     CA|20581.0|  25.0|    2.0|  1.0|20160428|     DOH| null|      G|      O|   null|      M| 1991.0|10272016|     M|  null|     QR| 9.478969603E10|00739|      B2|\n",
      "| 985523.0|2016.0|   4.0| 111.0| 111.0|    CHM|20550.0|    3.0|     NY|20553.0|  19.0|    2.0|  1.0|20160406|    null| null|      Z|      K|   null|      M| 1997.0|07042016|     F|  null|   null|4.2322572633E10| LAND|      WT|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read in immigration data\n",
    "fname = 'immigration_data_sample.csv'\n",
    "immigrationSample = spark.read.csv(fname,header=True,inferSchema=True)\n",
    "\n",
    "immigrationSample = immigrationSample.drop('_c0')\n",
    "immigrationSample.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Raw Immigration Data Dictionary\n",
    "\n",
    "| Column Name  | Description |\n",
    "| ------------ | ----------------------------------------- |\n",
    "| cicid        | identifier                                |\n",
    "| i94yr        | 4 digit year                              |\n",
    "| i94mon       | Numeric month                             |\n",
    "| i94cit       | code for immigrant's birth country        |\n",
    "| i94res       | code for immigrant's country of residence |\n",
    "| i94port      | code for admission port                   |\n",
    "| arrdate      | arrival date                              |\n",
    "| i94mode      | mode of transportation                    |\n",
    "| i94addr      | arrival state                             |\n",
    "| depdate      | departure date                            |\n",
    "| i94bir       | age                                       |\n",
    "| i94visa      | visa code                                 |\n",
    "| count        | Used for summary statistics               |\n",
    "| dtadfile     | date added to file                        |\n",
    "| visapost     | post where visa was issued                |\n",
    "| occup        | occupation                                |\n",
    "| entdepa      | arrival flag                              |\n",
    "| entdepu      | departure flag                            |\n",
    "| matflag      | match flag                                |\n",
    "| biryear      | birth year                                |\n",
    "| dtaddto      | admission date                            |\n",
    "| gender       | gender                                    |\n",
    "| insnum       | INS number                                |\n",
    "| airline      | airline used to enter US                  |\n",
    "| admnum       | admission number                          |\n",
    "| fltno        | flight number                             |\n",
    "| visatype     | category of visa                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#read in data\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "temperature = spark.read.csv(fname,header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|                 dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+-------------------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01 00:00:00|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01 00:00:00|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01 00:00:00|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01 00:00:00|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01 00:00:00|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+-------------------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Raw Temperature Data Dictionary\n",
    "\n",
    "| Column Name                   | Description |\n",
    "| ----------------------------- | ---------------------------------------------- |\n",
    "| dt                            | date                                           |\n",
    "| AverageTemperature            | global average land temperature in celsius     |\n",
    "| AverageTemperatureUncertainty | the 95% confidence interval around the average |\n",
    "| City                          | name of city                                   |\n",
    "| Country                       | name of country                                |\n",
    "| Latitude                      | latitude of city                               |\n",
    "| Longitude                     | longitude of city                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#read in data\n",
    "fname = 'us-cities-demographics.csv'\n",
    "demographics = spark.read.csv(fname,header=True,sep=';',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Raw Demographics Data Dictionary\n",
    "\n",
    "| Column Name            | Description |\n",
    "| ---------------------- | -------------------------------------------------- |\n",
    "| City                   | name of city                                       |\n",
    "| State                  | state where city is located                        |\n",
    "| Median Age             | median age of city                                 |\n",
    "| Male Population        | count of males in the city                         |\n",
    "| Female Population      | count of females in the city                       |\n",
    "| Total Population       | count of all people in the city                    |\n",
    "| Number of Veterans     | count of veterans in the city                      |\n",
    "| Foreign-born           | count of people not born in the city               |\n",
    "| Average Household Size | average number of people per household in the city |\n",
    "| State Code             | code for the state                                 |\n",
    "| Race                   | respondent's race                                  |\n",
    "| Count                  | count by ethnicity for city                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#read in data\n",
    "fname = 'airport-codes_csv.csv'\n",
    "airport = spark.read.csv(fname,header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Raw Airport Data Dictionary\n",
    "\n",
    "| Column Name  | Description |\n",
    "| ------------ | -------------------------------------------------------------- |\n",
    "| ident        | identifier                                                     |\n",
    "| type         | description of type of airport (heliport, small_airport, etc.) |\n",
    "| name         | name of airport                                                |\n",
    "| elevation_ft | elevation level (in feet)                                      |\n",
    "| continent    | continent of airport location                                  |\n",
    "| iso_country  | country of airport location                                    |\n",
    "| iso_region   | region (i.e. state) of airport location                        |\n",
    "| municipality | city of airport location                                       |\n",
    "| gps_code     | GPS airport code                                               |\n",
    "| iata_code    | IATA airport code                                              |\n",
    "| local_code   | local airport code                                             |\n",
    "| latitude     | latitude of airport location                                    |\n",
    "| longitude    | longitude of airport location                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "In this section, we will identify data quality issues (missing values, duplicate data, etc.) and list any necessary steps taken to clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Immigration\n",
    "\n",
    "First, let's load in the immigration data set. This will be the foundation for  our fact table. \n",
    "\n",
    "We will discover that the immigration data is pretty clean as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read in data\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigration = spark.read.format(\"com.github.saurfang.sas.spark\").load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this shows that there's no data with all null records\n",
    "immigration.na.drop(how='all',subset=['cicid']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this shows that there's no duplicate data\n",
    "immigration.drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature\n",
    "\n",
    "We've already loaded in this data set above, so let's clean it up. \n",
    "\n",
    "You can observe straight away that there are records dating back to the 1700s. We won't want to include this data, because commerical airfare wasn't around back then. According to [this link](https://en.wikipedia.org/wiki/Airline#:~:text=Tony%20Jannus%20conducted%20the%20United,Petersburg%2DTampa%20Airboat%20Line.), the first commercial flight in the US occurred in 1914, so we will remove any data earlier than this. \n",
    "\n",
    "We only care about data for the United States, because that is where our immigration data is based. Thus, we can filter only for records from the US.\n",
    "\n",
    "In addition, we will remove NaN values from the table as it doesn't make sense to analyze the data without a reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop where AverageTemperature = NaN\n",
    "temperature = temperature.dropna(how='any',subset='AverageTemperature')\n",
    "\n",
    "#filter where Country = USA\n",
    "temperature = temperature.filter(temperature.Country==\"United States\")\n",
    "\n",
    "#filter where dt >= 1914\n",
    "temperature = temperature.filter(temperature.dt>=\"1914-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307628"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "|                 dt|AverageTemperature|AverageTemperatureUncertainty|   City|      Country|Latitude|Longitude|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "|1914-01-01 00:00:00| 9.802999999999999|                        0.856|Abilene|United States|  32.95N|  100.53W|\n",
      "|1914-02-01 00:00:00|              5.84|                        0.442|Abilene|United States|  32.95N|  100.53W|\n",
      "|1914-03-01 00:00:00|11.574000000000002|                        0.426|Abilene|United States|  32.95N|  100.53W|\n",
      "|1914-04-01 00:00:00|16.366999999999994|                        0.318|Abilene|United States|  32.95N|  100.53W|\n",
      "|1914-05-01 00:00:00|            20.002|                        0.556|Abilene|United States|  32.95N|  100.53W|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographics\n",
    "\n",
    "We will discover that the demographics data is pretty clean as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this shows there's no data with all null records\n",
    "demographics.na.drop(how='all').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this shows that there's no duplicate data\n",
    "demographics.drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airport\n",
    "\n",
    "We've already loaded in this data set above, so let's clean it up.\n",
    "\n",
    "We are going to shard out the coordinates by latitude and longitude, in order to be consistent with temperature table.\n",
    "\n",
    "We only care about data for the United States, because that is where our immigration data is based. Thus, we can filter only for records from the US.\n",
    "\n",
    "We can also remove the 'US-' portion of the iso_region (state code) as that is duplicate information to the iso_country column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# separate coordinates\n",
    "airport = airport.withColumn(\"coordinates\",split(\"coordinates\",\",\"))\n",
    "airport = airport.withColumn(\"latitude\",airport.coordinates[0]).withColumn(\"longitude\",airport.coordinates[1])\n",
    "airport = airport.drop(\"coordinates\")\n",
    "\n",
    "# filter where iso_country = US\n",
    "airport = airport.filter(airport.iso_country==\"US\")\n",
    "\n",
    "# remove US- from iso_region\n",
    "airport = airport.withColumn(\"iso_region\",expr(\"substring(iso_region,4,length(iso_region))\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22757"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+------------------+------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|          latitude|         longitude|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+------------------+------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|        PA|    Bensalem|     00A|     null|       00A|-74.93360137939453|    40.07080078125|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|        KS|       Leoti|    00AA|     null|      00AA|       -101.473911|         38.704022|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|        AK|Anchor Point|    00AK|     null|      00AK|    -151.695999146|       59.94919968|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|        AL|     Harvest|    00AL|     null|      00AL|-86.77030181884766| 34.86479949951172|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|        AR|     Newport|    null|     null|      null|        -91.254898|           35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "For this project, we will create a star schema, using one fact table and five dimension tables. Since the goal of this project is to derive insights on immigration patterns to the US, we will use the immigration data set as the basis for our fact table. This table will be a subset of the \"raw\" immigration table from above.\n",
    "\n",
    "We will have one dimension table for each remaining data set above (temperature, demographics, airport), plus tables for time and coordinates. The time table will allow us to aggregate the data using various time units. The cooridnates table will be derived from the airport table and consist of the city, latitude, and longitude.\n",
    "\n",
    "Below is a visual of the conceptual data model:\n",
    "\n",
    "![Data Dictionary](DataDictionary.png)\n",
    "\n",
    "**Note:** You can also see this image in the file <code>DataDictionary.png</code>, located in this repository.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "We will define one Python function for the pipeline for each table. We will read in the data sets again, both for clarity and to include the cleaning steps from above (if any) in the full pipeline map. The following is how we will create each table:\n",
    "\n",
    "**immigration:**\n",
    "\n",
    "* load in immigration SAS file to Spark dataframe\n",
    "* select relevant columns\n",
    "* write to parquet\n",
    "\n",
    "**temperature:**\n",
    "\n",
    "* load in temperature CSV file to Spark dataframe\n",
    "* clean the data as per part 2\n",
    "* select relevant columns and normalize column names\n",
    "* write to parquet\n",
    "\n",
    "**demographics:**\n",
    "\n",
    "* load in demographics CSV file to Spark dataframe\n",
    "* select relevant columns and normalize column names\n",
    "* write to parquet\n",
    "\n",
    "**airport:**\n",
    "\n",
    "* load in airport CSV file to Spark dataframe\n",
    "* clean the data as per part 2\n",
    "* select relevant columns and normalize column names\n",
    "* write to parquet\n",
    "\n",
    "**coordinates:**\n",
    "\n",
    "* parse and add latitude/longitude columns as per part 2\n",
    "* select releavant columns and normalize column names\n",
    "* write to parquet\n",
    "\n",
    "**time:**\n",
    "\n",
    "* gather dates from immigration data\n",
    "* parse out year, month, day, weekday\n",
    "* write to parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#function for immigration fact table\n",
    "\n",
    "def create_immigration_table(df, output_data):\n",
    "    '''\n",
    "    Creates the immigration fact table.\n",
    "            Parameters:\n",
    "                df: Spark dataframe\n",
    "                output_data: location where results are stored\n",
    "            Returns:\n",
    "                None.\n",
    "    '''\n",
    "    \n",
    "    print(\"Begin pipeline for immigration table.\")\n",
    "    print(\"Select and normalize relevant columns.\")\n",
    "    \n",
    "    immigration_table = df.select(\"cicid\",\"i94yr\",\"i94mon\",\"i94cit\",\"i94res\",\"i94port\",\"arrdate\",\"i94mode\",\"i94addr\",\"depdate\",\"i94bir\",\"i94visa\",\"visapost\",\"entdepa\",\"entdepu\",\"matflag\",\"gender\",\"visatype\")\n",
    "    \n",
    "    # write to parquet file\n",
    "    immigration_output_location = output_data + \"immigration.parquet\"\n",
    "    immigration_table.write.parquet(immigration_output_location, mode=\"overwrite\")\n",
    "    \n",
    "    print(\"Wrote immigration table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#function for temperature dimension table\n",
    "\n",
    "def create_temperature_table(df, output_data):\n",
    "    '''\n",
    "    Creates the temperature dimension table.\n",
    "            Parameters:\n",
    "                df: Spark dataframe\n",
    "                output_data: location where results are stored\n",
    "            Returns:\n",
    "                None.\n",
    "    '''\n",
    "    \n",
    "    print(\"Begin pipeline for temperature table.\")\n",
    "    print(\"Cleaning temperature data.\")\n",
    "    \n",
    "    # drop where AverageTemperature = NaN\n",
    "    temperature_table = df.dropna(how='any',subset='AverageTemperature')\n",
    "    #filter where Country = USA\n",
    "    temperature_table = temperature_table.filter(temperature_table.Country==\"United States\")\n",
    "    #filter where dt >= 1914\n",
    "    temperature_table = temperature_table.filter(temperature_table.dt>=\"1914-01-01\")\n",
    "    \n",
    "    print(\"Select and normalize relevant columns.\")\n",
    "    \n",
    "    temperature_table = temperature_table.select(\"dt\",\"City\",\"AverageTemperature\",\"AverageTemperatureUncertainty\")\n",
    "    temperature_table = temperature_table.withColumnRenamed(\"City\",\"city\").withColumnRenamed(\"AverageTemperature\",\"avgTemp\").withColumnRenamed(\"AverageTemperatureUncertainty\",\"avgTempUncert\")\n",
    "    \n",
    "    # write to parquet file\n",
    "    temperature_output_location = output_data + \"temperature.parquet\"\n",
    "    temperature_table.write.parquet(temperature_output_location, mode=\"overwrite\")\n",
    "    \n",
    "    print(\"Wrote temperature table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#function for demographics dimension table\n",
    "\n",
    "def create_demographics_table(df, output_data):\n",
    "    '''\n",
    "    Creates the demographics dimension table.\n",
    "            Parameters:\n",
    "                df: Spark dataframe\n",
    "                output_data: location where results are stored\n",
    "            Returns:\n",
    "                None.\n",
    "    '''\n",
    "    \n",
    "    print(\"Begin pipeline for demographics table.\")\n",
    "    print(\"Select and normalize relevant columns.\")\n",
    "    \n",
    "    demographics_table = df.select(\"City\",\"State Code\",\"Median Age\",\"Male Population\",\"Female Population\",\"Total Population\",\"Number of Veterans\",\"Foreign-born\",\"Average Household Size\",\"Race\",\"Count\")\n",
    "    demographics_table = demographics_table.withColumnRenamed(\"City\",\"city\").withColumnRenamed(\"State Code\",\"state\").withColumnRenamed(\"Median Age\",\"medAge\").withColumnRenamed(\"Male Population\",\"malePopulation\").withColumnRenamed(\"Female Population\",\"femalePopulation\").withColumnRenamed(\"Total Population\",\"totPopulation\").withColumnRenamed(\"Number of Veterans\",\"numVeterans\").withColumnRenamed(\"Foreign-born\",\"foreignBorn\").withColumnRenamed(\"Average Household Size\",\"avgHouseholdSize\").withColumnRenamed(\"Race\",\"race\").withColumnRenamed(\"Count\",\"count\")\n",
    "\n",
    "    # write to parquet file\n",
    "    demographics_output_location = output_data + \"demographics.parquet\"\n",
    "    demographics_table.write.parquet(demographics_output_location, mode=\"overwrite\")\n",
    "    \n",
    "    print(\"Wrote demographics table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#function for airport dimension table\n",
    "\n",
    "def create_airport_table(df, output_data):\n",
    "    '''\n",
    "    Creates the airport dimension table.\n",
    "            Parameters:\n",
    "                df: Spark dataframe\n",
    "                output_data: location where results are stored\n",
    "            Returns:\n",
    "                None.\n",
    "    '''\n",
    "    \n",
    "    print(\"Begin pipeline for airport table.\")\n",
    "    print(\"Cleaning aiport data.\")\n",
    "    \n",
    "    # filter where iso_country = US\n",
    "    airport_table = df.filter(df.iso_country==\"US\")\n",
    "    # remove US- from iso_region\n",
    "    airport_table = airport_table.withColumn(\"iso_region\",expr(\"substring(iso_region,4,length(iso_region))\"))\n",
    "    \n",
    "    print(\"Select and normalize relevant columns.\")\n",
    "    \n",
    "    airport_table = airport_table.select(\"ident\",\"type\",\"name\",\"elevation_ft\",\"iso_region\",\"municipality\",\"gps_code\",\"iata_code\",\"local_code\")\n",
    "    airport_table = airport_table.withColumnRenamed(\"elevation_ft\",\"elevation\").withColumnRenamed(\"iso_region\",\"state\").withColumnRenamed(\"municipality\",\"city\")\n",
    "    \n",
    "    # write to parquet file\n",
    "    airport_output_location = output_data + \"airport.parquet\"\n",
    "    airport_table.write.parquet(airport_output_location, mode=\"overwrite\")\n",
    "    \n",
    "    print(\"Wrote airport table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#function for coordinates dimension table\n",
    "\n",
    "def create_coordinates_table(df, output_data):\n",
    "    '''\n",
    "    Creates the coordinates dimension table.\n",
    "            Parameters:\n",
    "                df: Spark dataframe\n",
    "                output_data: location where results are stored\n",
    "            Returns:\n",
    "                None.\n",
    "    '''\n",
    "    \n",
    "    print(\"Begin pipeline for coordinates table.\")\n",
    "    print(\"Cleaning coordinates data to create coordinates table.\")\n",
    "    \n",
    "    # separate coordinates\n",
    "    coordinates_table = df.withColumn(\"coordinates\",split(\"coordinates\",\",\"))\n",
    "    coordinates_table = coordinates_table.withColumn(\"latitude\",coordinates_table.coordinates[0]).withColumn(\"longitude\",coordinates_table.coordinates[1])\n",
    "    coordinates_table = coordinates_table.drop(\"coordinates\")\n",
    "    # filter where iso_country = US\n",
    "    coordinates_table = coordinates_table.filter(coordinates_table.iso_country==\"US\")\n",
    "    # remove US- from iso_region\n",
    "    coordinates_table = coordinates_table.withColumn(\"iso_region\",expr(\"substring(iso_region,4,length(iso_region))\"))\n",
    "    \n",
    "    print(\"Select and normalize relevant columns.\")\n",
    "    \n",
    "    coordinates_table = coordinates_table.select(\"municipality\",\"latitude\",\"longitude\")\n",
    "    coordinates_table = coordinates_table.withColumnRenamed(\"municipality\",\"city\")\n",
    "    coordinates_table = coordinates_table.withColumn('latitude',coordinates_table['latitude'].cast(\"double\").alias('latitude'))\n",
    "    coordinates_table = coordinates_table.withColumn('longitude',coordinates_table['longitude'].cast(\"double\").alias('longitude'))\n",
    "    \n",
    "    # write to parquet file\n",
    "    coordinates_output_location = output_data + \"coordinates.parquet\"\n",
    "    coordinates_table.write.parquet(coordinates_output_location, mode=\"overwrite\")\n",
    "    \n",
    "    print(\"Wrote coordinates table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#function for time dimension table\n",
    "\n",
    "def create_time_table(df, output_data):\n",
    "    '''\n",
    "    Creates the time dimension table.\n",
    "            Parameters:\n",
    "                df: Spark dataframe\n",
    "                output_data: location where results are stored\n",
    "            Returns:\n",
    "                None.\n",
    "    '''\n",
    "    \n",
    "    print(\"Begin pipeline for time table.\")\n",
    "    print(\"Select and normalize relevant columns.\")\n",
    "    \n",
    "    time_table = df.select(\"arrdate\")\n",
    "    int_to_date_udf = udf(lambda x: (dt.datetime(1900,1,1).date() + dt.timedelta(x)).isoformat())\n",
    "    time_table = time_table.withColumn(\"arrdate\", int_to_date_udf(\"arrdate\"))\n",
    "    time_table = time_table.withColumn(\"year\",year(\"arrdate\")).withColumn(\"month\",month(\"arrdate\")).withColumn(\"day\",dayofmonth(\"arrdate\")).withColumn(\"weekday\",dayofweek(\"arrdate\")).select(\"arrdate\",\"year\",\"month\",\"day\",\"weekday\").drop_duplicates()  \n",
    "    time_table = time_table.withColumnRenamed(\"arrdate\",\"date\")\n",
    "    \n",
    "    # write to parquet file\n",
    "    time_output_location = output_data + \"time.parquet\"\n",
    "    time_table.write.parquet(time_output_location, mode=\"overwrite\")\n",
    "    \n",
    "    print(\"Wrote time table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#read in all data\n",
    "\n",
    "immigration_fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigration = spark.read.format(\"com.github.saurfang.sas.spark\").load(immigration_fname)\n",
    "\n",
    "temperature_fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "temperature = spark.read.csv(temperature_fname,header=True,inferSchema=True)\n",
    "\n",
    "demographics_fname = 'us-cities-demographics.csv'\n",
    "demographics = spark.read.csv(demographics_fname,header=True,sep=';',inferSchema=True)\n",
    "\n",
    "airport_fname = 'airport-codes_csv.csv'\n",
    "airport = spark.read.csv(airport_fname,header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin pipeline for immigration table.\n",
      "Select and normalize relevant columns.\n",
      "Wrote immigration table.\n",
      "Begin pipeline for temperature table.\n",
      "Cleaning temperature data.\n",
      "Select and normalize relevant columns.\n",
      "Wrote temperature table.\n",
      "Begin pipeline for demographics table.\n",
      "Select and normalize relevant columns.\n",
      "Wrote demographics table.\n",
      "Begin pipeline for airport table.\n",
      "Cleaning aiport data.\n",
      "Select and normalize relevant columns.\n",
      "Wrote airport table.\n",
      "Begin pipeline for coordinates table.\n",
      "Cleaning coordinates data to create coordinates table.\n",
      "Select and normalize relevant columns.\n",
      "Wrote coordinates table.\n",
      "Begin pipeline for time table.\n",
      "Select and normalize relevant columns.\n",
      "Wrote time table.\n"
     ]
    }
   ],
   "source": [
    "#run each function using Spark dataframes defined above\n",
    "\n",
    "output_data = \"tables/\"\n",
    "\n",
    "create_immigration_table(immigration, output_data)\n",
    "create_temperature_table(temperature, output_data)\n",
    "create_demographics_table(demographics, output_data)\n",
    "create_airport_table(airport, output_data)\n",
    "create_coordinates_table(airport, output_data)\n",
    "create_time_table(immigration, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    " \n",
    "Let's run some data quality checks to ensure the pipeline ran as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#function for data quality\n",
    "\n",
    "def data_quality_checks(table_names):\n",
    "    '''\n",
    "    Runs data quality checks on the pipeline.\n",
    "        Parameters:\n",
    "            table_name: list of table names\n",
    "        Returns:\n",
    "            None.\n",
    "    '''\n",
    "    \n",
    "    print(\"Begin data quality checks.\")\n",
    "    \n",
    "    list_of_tables = os.listdir(\"tables\")\n",
    "    \n",
    "    for table in table_names:\n",
    "        file_name = table+\".parquet\"\n",
    "        if (file_name in list_of_tables):\n",
    "            print(\"Data quality existence check PASSED for \"+table)\n",
    "        else:\n",
    "            print(\"Data quality existence check FAILED for \"+table)\n",
    "        \n",
    "    for table in table_names:\n",
    "        data = spark.read.parquet(\"./tables/\"+table+\".parquet\")\n",
    "        data.createOrReplaceTempView(\"data\")\n",
    "        tableCount = spark.sql(\"SELECT COUNT(*) FROM data\")\n",
    "        if tableCount == 0:\n",
    "            print(\"Data quality count check FAILED for \"+table)\n",
    "        else:\n",
    "            print(\"Data quality count check PASSED for \"+table)\n",
    "        \n",
    "    print(\"Data quality checks complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin data quality checks.\n",
      "Data quality existence check PASSED for immigration\n",
      "Data quality existence check PASSED for temperature\n",
      "Data quality existence check PASSED for demographics\n",
      "Data quality existence check PASSED for airport\n",
      "Data quality existence check PASSED for coordinates\n",
      "Data quality existence check PASSED for time\n",
      "Data quality count check PASSED for immigration\n",
      "Data quality count check PASSED for temperature\n",
      "Data quality count check PASSED for demographics\n",
      "Data quality count check PASSED for airport\n",
      "Data quality count check PASSED for coordinates\n",
      "Data quality count check PASSED for time\n",
      "Data quality checks complete.\n"
     ]
    }
   ],
   "source": [
    "table_names = [\"immigration\", \"temperature\", \"demographics\", \"airport\", \"coordinates\", \"time\"]\n",
    "data_quality_checks(table_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary\n",
    "\n",
    "Below is the data dictionary:\n",
    "\n",
    "![Data Dictionary](DataDictionary.png)\n",
    "\n",
    "**Note:** You can also see the data dictionary in the file <code>DataDictionary.png</code>, located in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "\n",
    "#### Rationale for Choice of Tools/Technologies:\n",
    "\n",
    "Apache Spark was the main technology used for this project. Spark is ideal for rapid querying, analysis, and transformation of large data sets, as well as simplifying complex data pipelines. Given that the immigration data set was over 3 million rows alone for only one month of information, in combination with the temperature, airport, and demographics data sets, Spark was the best choice.\n",
    "\n",
    "#### Frequency of Updates:\n",
    "\n",
    "The data should be updated every month at most. Given that we only receive a temperature \n",
    "\n",
    "#### Scenarios To Address:\n",
    "\n",
    "Problems constantly arise when dealing with databases. The underlying infrastructure must be equipped to handle such situations without major disruption to the end user. Below are example scenarios with an approach that ensure no major issues develop:\n",
    "\n",
    "**1. Data volume increased by 100x:** Because we are using Spark, we can increase the number of nodes in order to handle the load. <br/><br/>\n",
    "**2. Pipelines ran daily at 7am:** The user of Apache Airflow would be beneficial here. Apache Airflow is an open-source tool which structures data pipelines as DAGs that can run on a schedule. For this particular pipeline, a user could configure Airflow to run daily at 7am, in which it would stream the data from the source and update the tables accordingly. <br/><br/>\n",
    "**3. Database needs access by 100+ people:** We can host the database in Amazon Redshift, which supports up to 500 concurrent connections (source: https://docs.aws.amazon.com/redshift/latest/mgmt/amazon-redshift-limits.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
